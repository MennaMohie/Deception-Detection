{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"Extracting region of interest.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"AZS3IuPu_ZvN","colab_type":"text"},"source":["<h1>Extracting region of interest from each video (person)</h1>"]},{"cell_type":"markdown","metadata":{"id":"qvFQ6Doy_ZvR","colab_type":"text"},"source":["<h2>Imports</h2>"]},{"cell_type":"code","metadata":{"id":"NtuYFpwb_ZvT","colab_type":"code","outputId":"6aff701c-4045-44ed-afc0-1f04ceb32dbb","executionInfo":{"status":"ok","timestamp":1585381041881,"user_tz":-120,"elapsed":228812,"user":{"displayName":"Menna Mohie El-deen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh2DC4A171YiJjZbFMozg6_v4H7LsmV-_GOnmT1AzA=s64","userId":"17611062510662143796"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!pip install cvlib\n","!pip uninstall tensorflow\n","!pip install tensorflow==2.0.0\n","import numpy as np\n","import cv2\n","import glob\n","import cvlib as cv\n","from cvlib.object_detection import draw_bbox\n","from google.colab import drive\n","from google.colab import files\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting cvlib\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/31/4d52c061ecfb70839439e22683fc7d114cbc16fad5220d46c774ff9c9bfb/cvlib-0.2.4.tar.gz (10.0MB)\n","\u001b[K     |████████████████████████████████| 10.1MB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from cvlib) (1.18.2)\n","Collecting progressbar\n","  Downloading https://files.pythonhosted.org/packages/a3/a6/b8e451f6cff1c99b4747a2f7235aa904d2d49e8e1464e0b798272aa84358/progressbar-2.5.tar.gz\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from cvlib) (2.21.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from cvlib) (7.0.0)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (from cvlib) (2.4.1)\n","Requirement already satisfied: imutils in /usr/local/lib/python3.6/dist-packages (from cvlib) (0.5.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->cvlib) (2019.11.28)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->cvlib) (2.8)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->cvlib) (1.24.3)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->cvlib) (3.0.4)\n","Building wheels for collected packages: cvlib, progressbar\n","  Building wheel for cvlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for cvlib: filename=cvlib-0.2.4-cp36-none-any.whl size=10043766 sha256=c993ba8e36ca677e9d6c3ec8b71a552ff9137b49cc75813bb5aae9083d3634bc\n","  Stored in directory: /root/.cache/pip/wheels/e3/2b/5e/da538fa7a4fc51ba1f3e89c57f678f9ebfd1247b9b70c80a5a\n","  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for progressbar: filename=progressbar-2.5-cp36-none-any.whl size=12074 sha256=8cc0fe9b6fb94f6454d97dc4f567308ab2cb63aa1a23148e78cf86121ba440f8\n","  Stored in directory: /root/.cache/pip/wheels/c0/e9/6b/ea01090205e285175842339aa3b491adeb4015206cda272ff0\n","Successfully built cvlib progressbar\n","Installing collected packages: progressbar, cvlib\n","Successfully installed cvlib-0.2.4 progressbar-2.5\n","Uninstalling tensorflow-2.2.0rc1:\n","  Would remove:\n","    /usr/local/bin/estimator_ckpt_converter\n","    /usr/local/bin/saved_model_cli\n","    /usr/local/bin/tensorboard\n","    /usr/local/bin/tf_upgrade_v2\n","    /usr/local/bin/tflite_convert\n","    /usr/local/bin/toco\n","    /usr/local/bin/toco_from_protos\n","    /usr/local/lib/python3.6/dist-packages/tensorflow-2.2.0rc1.dist-info/*\n","    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n","Proceed (y/n)? y\n","  Successfully uninstalled tensorflow-2.2.0rc1\n","Collecting tensorflow==2.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n","\u001b[K     |████████████████████████████████| 86.3MB 37kB/s \n","\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.12.1)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.2.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.9.0)\n","Collecting tensorboard<2.1.0,>=2.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 36.7MB/s \n","\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.34.2)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.12.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.27.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.2.0)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.0.8)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.8.1)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.10.0)\n","Collecting tensorflow-estimator<2.1.0,>=2.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n","\u001b[K     |████████████████████████████████| 450kB 59.7MB/s \n","\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.18.2)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n","Collecting gast==0.2.2\n","  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (46.0.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.0.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.21.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.2.1)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.7.2)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0) (2.10.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.3.0)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.8)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.24.3)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2019.11.28)\n","Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.2.8)\n","Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.8)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=9c35deb28aea4e9bd9e274f22980250d3eb9e8d240fee40e0e1864bd9b8fbeec\n","  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n","Successfully built gast\n","Installing collected packages: tensorboard, tensorflow-estimator, gast, tensorflow\n","  Found existing installation: tensorboard 2.1.1\n","    Uninstalling tensorboard-2.1.1:\n","      Successfully uninstalled tensorboard-2.1.1\n","  Found existing installation: tensorflow-estimator 2.2.0rc0\n","    Uninstalling tensorflow-estimator-2.2.0rc0:\n","      Successfully uninstalled tensorflow-estimator-2.2.0rc0\n","  Found existing installation: gast 0.3.3\n","    Uninstalling gast-0.3.3:\n","      Successfully uninstalled gast-0.3.3\n","Successfully installed gast-0.2.2 tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1\n","Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OOf3oFwM_ZvY","colab_type":"text"},"source":["<h2>Helper functions</h2>\n","\n","This function loads the videos from a specified path and returns them in a list."]},{"cell_type":"code","metadata":{"id":"b-6hCLYJ_ZvZ","colab_type":"code","colab":{}},"source":["def read_videos(path):\n","    videos = []\n","    videos_paths = glob.glob(path + '/*[0-9].mp4')\n","    for video_path in videos_paths:\n","      videos.append(cv2.VideoCapture(video_path))\n","    return videos"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HFmsWPZq_Zvd","colab_type":"text"},"source":["Next, we implement the function that applies YOLO to find the bounding boxes of the persons in one video.\n","\n","This function returns a list containing the bounding boxes of each frame in one video.\n","If more than one bounding box are found in one frame, the bouding boxes are merged to obtain one bounding box."]},{"cell_type":"code","metadata":{"id":"aWY1kBd3_Zve","colab_type":"code","colab":{}},"source":["def yolo(video):\n","  frames_no = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n","  bounding_boxes = []\n","  print('start')\n","  for frame_no in range(frames_no):\n","    ret, frame = video.read()\n","    if ret != True:\n","      continue\n","    bboxes, label, confidence = cv.detect_common_objects(frame)\n","    if len(bboxes) == 0:\n","      continue\n","    x_top_left = bboxes[0][0]\n","    y_top_left = bboxes[0][1]\n","    x_bottom_right = bboxes[0][2]\n","    y_bottom_right = bboxes[0][3]\n","    for i in range(1, len(bboxes)): # if more than one bbox, merge them\n","      x_top_left = min(x_top_left, bboxes[i][0])\n","      y_top_left = min(y_top_left, bboxes[i][1])\n","      x_bottom_right = max(x_bottom_right, bboxes[i][2])\n","      y_bottom_right = max(y_bottom_right, bboxes[i][3])\n","    bbox = [x_top_left, y_top_left, x_bottom_right, y_bottom_right]\n","    bounding_boxes.append(bbox)\n","    return bounding_boxes"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UE-PfcNFToha","colab_type":"text"},"source":["This function finds the bounding box with the largest area."]},{"cell_type":"code","metadata":{"id":"cOBesfQX_Zvj","colab_type":"code","colab":{}},"source":["def get_max_min_bounding_boxes(bounding_boxes):\n","  max_bbox = []\n","  max_bbox_area = -1\n","  min_bbox = []\n","  min_bbox_area = 1000000000000000\n","  for bbox in bounding_boxes:\n","    width = bbox[2] - bbox[0]\n","    height = bbox[3] - bbox[1]\n","    bbox_area = width * height\n","    if bbox_area > max_bbox_area:\n","      max_bbox_area = bbox_area\n","      max_bbox = bbox\n","    if bbox_area < min_bbox_area:\n","      min_bbox_area = bbox_area\n","      min_bbox = bbox\n","  return max_bbox, max_bbox_area, min_bbox, min_bbox_area"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T64lWeDwTu5T","colab_type":"text"},"source":["This function crops one video according to the input bounding_box. It also resizes the video to the required size."]},{"cell_type":"code","metadata":{"id":"6Z-CXD6u_Zvn","colab_type":"code","colab":{}},"source":["def crop_video(video, bounding_box):\n","    x_top_left = int(max(0, bounding_box[0]))\n","    y_top_left = int(max(0, bounding_box[1]))\n","    x_bottom_right = int(min(video.get(3) - 1, bounding_box[2])) # video.get(3) = width\n","    y_bottom_right = int(min(video.get(4) - 1, bounding_box[3])) # video.get(4) = height\n","    frames_no = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n","    cropped_frames = []\n","    for frame_no in range(frames_no):\n","        ret, frame = video.read()\n","        if ret != True:\n","          continue\n","        cropped_frame = frame[y_top_left:y_bottom_right, x_top_left:x_bottom_right]\n","        cropped_frames.append(cropped_frame)\n","    return cropped_frames"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-SnlXco_9LrS","colab_type":"code","colab":{}},"source":["def find_min_frames(videos):\n","  min_frames = 10000000\n","  for video in videos:\n","    frames_no = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n","    if frames_no < min_frames:\n","      min_frames = frames_no\n","  return min_frames"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xCPaf70O_Zvs","colab_type":"text"},"source":["<h2>Reading videos</h2>"]},{"cell_type":"code","metadata":{"id":"ZhtSDBel_Zvt","colab_type":"code","colab":{}},"source":["def read():\n","  path = '/content/gdrive/My Drive/Team\\'s Drive/Graduation Project/Project/Dataset/Clips'\n","  truthful_videos = read_videos(path + '/Truthful')\n","  deceptive_videos = read_videos(path + '/Deceptive')\n","  return truthful_videos, deceptive_videos"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"15tweHY3_Zvw","colab_type":"text"},"source":["<h3>Applying YOLO to get the bounding boxes per frame per video.</h3>\n","\n","Example:\n","To access the bounding boxes of the second frame in the first truthful video: truthful_boundingboxes[0][1] (list)"]},{"cell_type":"markdown","metadata":{"id":"YEiqSjnL_Zv0","colab_type":"text"},"source":["<h3>Finding the maximum possible bounding box for each video.</h3>"]},{"cell_type":"code","metadata":{"id":"FCgI7mgk_Zv4","colab_type":"code","colab":{}},"source":["truthful_videos, deceptive_videos = read()\n","max_bboxes_true = []\n","max_bboxes_lie = []\n","for i in range(len(truthful_videos)):\n","  vid = truthful_videos[i]\n","  bboxes = yolo(vid)\n","  max_bbox, max_area, min_bbox, min_area = get_max_min_bounding_boxes(bboxes)\n","  max_bboxes_true.append(max_bbox)\n","  #with open('true_bboxes/true' + str(i) + '.txt', 'w') as f:\n","  # f.write(np.array2string(np.asarray(max_bbox + min_bbox)))\n","\n","for i in range(len(deceptive_videos)):\n","  vid = deceptive_videos[i]\n","  bboxes = yolo(vid)\n","  max_bbox, max_area, min_bbox, min_area = get_max_min_bounding_boxes(bboxes)\n","  max_bboxes_lie.append(max_bbox)\n","  #with open('lie_bboxes/lie' + str(i) + '.txt', 'w') as f:\n","  #  f.write(np.array2string(np.asarray(max_bbox + min_bbox)))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lGd2gY4hM3Si","colab_type":"code","colab":{}},"source":["#!zip -r /content/true_bboxes.zip /content/true_bboxes\n","#!zip -r /content/lie_bboxes.zip /content/lie_bboxes"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"88-PFoaphCeZ","colab_type":"code","colab":{}},"source":["import os\n","truthful_videos, deceptive_videos = read()\n","min_true = find_min_frames(truthful_videos)\n","min_lie = find_min_frames(deceptive_videos)\n","minn = min(min_true, min_lie)\n","truthful_videos, deceptive_videos = read()\n","os.mkdir('Truthful')\n","for i in range(len(truthful_videos)):\n","    video = truthful_videos[i]\n","    cropped_frames = crop_video(video, max_bboxes_true[i])\n","    folder_name = 'Truthful/trial_truth_' + str(i + 1).zfill(3)\n","    os.mkdir(folder_name)\n","    if len(cropped_frames) <= minn:\n","      for j in range(len(cropped_frames)):\n","        num = str(j).zfill(3)\n","        cv2.imwrite(folder_name + '/' + num + '.jpg', cropped_frames[j])\n","    else:\n","      sub_vids = len(cropped_frames) // minn\n","      for k in range(sub_vids):\n","        subfolder_name = str(k).zfill(0)\n","        new_dir = folder_name + '/' + subfolder_name \n","        os.mkdir(new_dir)\n","        for j in range(minn):\n","          index = j + k * minn\n","          num = str(index).zfill(3)\n","          cv2.imwrite(new_dir + '/' + num + '.jpg', cropped_frames[index])\n","      remainder = len(cropped_frames) % minn\n","      k += 1\n","      if remainder > 0.5 * minn:\n","        missing = minn - remainder\n","        start_idx = len(cropped_frames) - remainder - missing\n","        subfolder_name = str(k).zfill(0)\n","        new_dir = folder_name + '/' + subfolder_name \n","        os.mkdir(new_dir)\n","        while start_idx < len(cropped_frames):\n","          num = str(start_idx).zfill(3)\n","          cv2.imwrite(new_dir + '/' + num + '.jpg', cropped_frames[start_idx])\n","          start_idx += 1\n","        k += 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6l9jS0KKhgR4","colab_type":"code","colab":{}},"source":["truthful_videos, deceptive_videos = read()\n","os.mkdir('Deceptive')\n","for i in range(len(deceptive_videos)):\n","    video = deceptive_videos[i]\n","    cropped_frames = crop_video(video, max_bboxes_lie[i])\n","    folder_name = 'Deceptive/trial_lie_' + str(i + 1).zfill(3)\n","    os.mkdir(folder_name)\n","    if len(cropped_frames) <= minn:\n","      for j in range(len(cropped_frames)):\n","        num = str(j).zfill(3)\n","        cv2.imwrite(folder_name + '/' + num + '.jpg', cropped_frames[j])\n","    else:\n","      sub_vids = len(cropped_frames) // minn\n","      for k in range(sub_vids):\n","        subfolder_name = str(k).zfill(0)\n","        new_dir = folder_name + '/' + subfolder_name \n","        os.mkdir(new_dir)\n","        for j in range(minn):\n","          index = j + k * minn\n","          num = str(index).zfill(3)\n","          cv2.imwrite(new_dir + '/' + num + '.jpg', cropped_frames[index])\n","      remainder = len(cropped_frames) % minn\n","      k += 1\n","      if remainder > 0.5 * minn:\n","        missing = minn - remainder\n","        start_idx = len(cropped_frames) - remainder - missing\n","        subfolder_name = str(k).zfill(0)\n","        new_dir = folder_name + '/' + subfolder_name \n","        os.mkdir(new_dir)\n","        while start_idx < len(cropped_frames):\n","          num = str(start_idx).zfill(3)\n","          cv2.imwrite(new_dir + '/' + num + '.jpg', cropped_frames[start_idx])\n","          start_idx += 1\n","        k += 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"l4muRrhgLkPI","colab_type":"code","colab":{}},"source":["!cp -r /content/Truthful /content/gdrive/\"My Drive\"/\"Team's Drive\"/\"Graduation Project\"/Project/Dataset/Cropped\n","!cp -r /content/Deceptive /content/gdrive/\"My Drive\"/\"Team's Drive\"/\"Graduation Project\"/Project/Dataset/Cropped"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PjqgL9D6PcD5","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yYxwwpzPSId5","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}