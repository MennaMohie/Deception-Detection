{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Deception Classifier - 3D CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3nSiVf1tBpfL",
        "BBe5Y5o5NA_P",
        "ouW1yxd1NVW_",
        "5jc6sgH4rtHG",
        "H9KG3yNcRCL3",
        "wLiPgRredZGL",
        "6eTzesIKBM2j",
        "RjAz5IOM9gO-",
        "jQlcIXNe4LrT",
        "mnHW5wXL5GjO",
        "RTmf4Hx66hCf",
        "DN60CYQu6wkV",
        "mb1A9Nrr7QjM",
        "HqWGduWl7Zw0",
        "FWYmqjv-78S1",
        "B1tCNpXo1O2c"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nSiVf1tBpfL",
        "colab_type": "text"
      },
      "source": [
        "# Required installations\n",
        "\n",
        "Run once."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PjOfiRhBnKP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install cvlib\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4nKu32e7aF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# When using for 1the first time make sure you\n",
        "#         log in with the teams account when the confirmation URL is requested.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBe5Y5o5NA_P",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xUAuVmRox_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import glob\n",
        "import cvlib as cv\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from keras import Model\n",
        "import tensorflow as tf\n",
        "#from tensorflow import keras\n",
        "import keras #Need to be solved\n",
        "from keras.layers import (Activation, Conv3D, Dense, Dropout, Flatten, \n",
        "                          MaxPooling3D, Lambda)\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.models import Sequential\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import np_utils\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import scipy\n",
        "from scipy import ndimage\n",
        "from random import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sz6ClmkRRT35",
        "colab_type": "text"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouW1yxd1NVW_",
        "colab_type": "text"
      },
      "source": [
        "## Loading Videos\n",
        "\n",
        "This functions loads the videos and returns them in a list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6s25IXYNX6Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_videos(video_paths: list) -> list:\n",
        "    videos = []\n",
        "    for video_path in video_paths:\n",
        "      videos.append(cv2.VideoCapture(video_path))\n",
        "    cv2.destroyAllWindows()\n",
        "    return videos"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jc6sgH4rtHG",
        "colab_type": "text"
      },
      "source": [
        "## Label Extraction\n",
        "\n",
        "* based on the videos names we assign a label, 0 for deceptive and 1 for truthful.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yts6at_drtvQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def video_labeling(video_paths: list) -> list:\n",
        "    video_label = []\n",
        "    for video_path in video_paths:\n",
        "        if 'lie' in video_path:\n",
        "            video_label.append(0)\n",
        "        else:\n",
        "            video_label.append(1)\n",
        "    return video_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9KG3yNcRCL3",
        "colab_type": "text"
      },
      "source": [
        "## Converting RGB videos to gray-scale\n",
        "\n",
        "this function takes a list of videos and returns a list of gray-scale videos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpCOtQ379hJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rgb2gray(rgb_videos: list, resize_shape: tuple=(40, 60)) -> list:\n",
        "    gray_videos = []\n",
        "    for video in rgb_videos:\n",
        "        # create a new video to act as the output gray video.\n",
        "        \n",
        "        # Read the first frame to find the original video attributes.\n",
        "        ret, first_frame = video.read()\n",
        "        cur_video_frames = []\n",
        "        while(video.isOpened()):\n",
        "            # Reading the next frame.\n",
        "            ret, cur_frame = video.read()\n",
        "            if not ret: \n",
        "                break \n",
        "            # Convert the RGB frame to gray-scale.\n",
        "            cur_frame = cv2.cvtColor(cur_frame, cv2.COLOR_BGR2GRAY)\n",
        "            # resizing the frame to (60 * 40)\n",
        "            cur_frame = cv2.resize(cur_frame, resize_shape)\n",
        "            # write the converted gray frame into the output video.\n",
        "            cur_video_frames.append(cur_frame.tolist())\n",
        "            # show the converted frame for testing\n",
        "            #cv2_imshow(cur_frame)\n",
        "\n",
        "            # to insure that there's more frames to be read.\n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                break\n",
        "        # closing the opened video.\n",
        "        video.release()\n",
        "        # delete all opened frames.\n",
        "        cv2.destroyAllWindows()\n",
        "        # append the converted video to the list of videos to be returned.\n",
        "        gray_videos.append(cur_video_frames)\n",
        "\n",
        "    return gray_videos\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLiPgRredZGL",
        "colab_type": "text"
      },
      "source": [
        "## Crop a Layer dimension\n",
        "Crops (or slices) a Tensor on a given dimension from start to end\n",
        "\n",
        "example : to crop tensor x[:, :, 5:10] call slice(2, 5, 10) as you want to crop on the third dimension"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rS7tWrEdZo-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crop(dimension, start, end):\n",
        "    # Crops (or slices) a Tensor on a given dimension from start to end\n",
        "    # example : to crop tensor x[:, :, 5:10]\n",
        "    # call slice(2, 5, 10) as you want to crop on the third dimension\n",
        "    def func(x):\n",
        "        if dimension == 0:\n",
        "            return x[start: end]\n",
        "        if dimension == 1:\n",
        "            return x[:, start: end]\n",
        "        if dimension == 2:\n",
        "            return x[:, :, start: end]\n",
        "        if dimension == 3:\n",
        "            return x[:, :, :, start: end]\n",
        "        if dimension == 4:\n",
        "            return x[:, :, :, :, start: end]\n",
        "    return Lambda(func)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoKn4iiD9dZe",
        "colab_type": "text"
      },
      "source": [
        "# Data Loading\n",
        "\n",
        "Defining the dataset path.\n",
        "\n",
        "\n",
        "loading the videos names.\n",
        "\n",
        "calling the read_videos function to read the videos into respective folders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eTzesIKBM2j",
        "colab_type": "text"
      },
      "source": [
        "## Loading the Old Dataset Format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTwzqGkjNFVW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Will be modified after dividing the videos into train and test\n",
        "# Drives' path.\n",
        "path = os.getcwd() + \"/gdrive/My Drive/Team's Drive/Graduation Project/Project/Dataset/Clips/\"\n",
        "\n",
        "# Deceptive and truthful folder's pathes\n",
        "deceptive_path = path + \"Deceptive/\"\n",
        "truthful_path  = path + \"Truthful/\"\n",
        "\n",
        "# Loading the videos names with their extension from the drive.\n",
        "total_videos_batch = glob.glob(deceptive_path + '*[0-9].mp4') + glob.glob(truthful_path + '*[0-9].mp4')\n",
        "\n",
        "# Shuffling the read videos to insure diversity.\n",
        "shuffle(total_videos_batch)\n",
        "\n",
        "# Getting the label column for the read videos.\n",
        "video_label = video_labeling(total_videos_batch)\n",
        "\n",
        "# Loading the actual vidios\n",
        "input_videos = read_videos(total_videos_batch)\n",
        "\n",
        "# converting the original RGB input videos to Gray-scale.\n",
        "input_videos = rgb2gray(input_videos[:2])\n",
        "#input_videos = rgb2gray(input_videos[:1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQUVxMFevUIg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_videos = read_videos(total_videos_batch)\n",
        "input_videos = rgb2gray(input_videos[:2])\n",
        "\n",
        "print(\"videos\", type(input_videos))\n",
        "print(\"frames\", type(input_videos[0]))\n",
        "print(\"rows\", type(input_videos[0][0]), len(input_videos[0][0]))\n",
        "print(\"cols\", type(input_videos[0][0][0]), len(input_videos[0][0][0]))\n",
        "print(type(input_videos[0][0][0][0]))\n",
        "\n",
        "input_videos = np.asarray(input_videos) # Invalid, different frames number\n",
        "print(input_videos.shape)\n",
        "print(type(input_videos[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjAz5IOM9gO-",
        "colab_type": "text"
      },
      "source": [
        "# Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQlcIXNe4LrT",
        "colab_type": "text"
      },
      "source": [
        "## Hard-wired Layer\n",
        "\n",
        "* Take steps of 7 frames.\n",
        "    \n",
        "    * Copy volume of gray frames. *(7 feature maps)*\n",
        "    * Apply gradient filter on the x-axis per frame. *(7 feature maps)*\n",
        "    * Apply gradient filter on the y-axis per frame. *(7 feature maps)*\n",
        "    * Extract optical flow channels for each 2 consecutive frames in both x and y. *(6 feature maps each)*\n",
        "    * Concatenate the resulting volumes, 5 channels.\n",
        "**Input dimensions (7@60x40)**\n",
        "\n",
        "**Output dimensions (33@60x40)**, 33 feature maps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2Ne5H_OZMIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def H1(cur_volume: np.ndarray(shape=(7, 60, 40))):\n",
        "    volume = cur_volume\n",
        "    prev_frame = None\n",
        "    for frame in cur_volume:\n",
        "        # For each frame, calculate the frame gradients\n",
        "        \"\"\"\n",
        "        # Reshaping the frame to 4D tensorflow onject.\n",
        "        tf_frame = tf.reshape(frame, shape = (1, frame.shape[1], frame.shape[0], 1))\n",
        "        # Using the tensorflow built-in image_gradients function.\n",
        "        g_x, g_y = tf.image.image_gradients(tf_frame)\n",
        "\n",
        "        # Convert the tensorflow object to a numpy array to manipulate it easily.\n",
        "        g_x = g_x.eval(session=tf.compat.v1.Session())\n",
        "        # Reshape the numpy array to have the channels first.\n",
        "        g_x = np.reshape(g_x, (1, 60, 40))\n",
        "        # Append the current x_gradient to the output volume.\n",
        "        volume = np.concatenate((volume, g_x), axis=0)\n",
        "\n",
        "        # Convert the tensorflow object to a numpy array to manipulate it easily.\n",
        "        g_y = g_y.eval(session=tf.compat.v1.Session())\n",
        "        # Reshape the numpy array to have the channels first.\n",
        "        g_y = np.reshape(g_y, (1, 60, 40))\n",
        "        # Append the current y_gradient to the output volume.\n",
        "        volume = np.concatenate((volume, g_y),  axis=0)\n",
        "        \"\"\"\n",
        "\n",
        "        # Using the scipy library to get the image gradient in the x-axis.\n",
        "        g_x = ndimage.sobel(frame, axis=0, mode='constant')\n",
        "        #cv2_imshow(g_x)\n",
        "        # Reshape the numpy array to have the channels first.\n",
        "        g_x = np.reshape(g_x, (1, g_x.shape[0], g_x.shape[1]))\n",
        "        # Append the current x_gradient to the output volume.\n",
        "        volume = np.concatenate((volume, g_x), axis=0)\n",
        "\n",
        "        # Using the scipy library to get the image gradient in the y-axis.\n",
        "        g_y = ndimage.sobel(frame, axis=1, mode='constant')\n",
        "        #cv2_imshow(g_y)\n",
        "        # Reshape the numpy array to have the channels first.\n",
        "        g_y = np.reshape(g_y, (1, g_y.shape[0], g_y.shape[1]))\n",
        "        # Append the current y_gradient to the output volume.\n",
        "        volume = np.concatenate((volume, g_y),  axis=0)\n",
        "\n",
        "    # Extract the optical flow maps on the x-axis.\n",
        "    prev_frame = None\n",
        "    for frame in cur_volume:\n",
        "        # For every 2 fames calculate the optical flow map between them.\n",
        "        if prev_frame is not None:\n",
        "            # Using the cv2 built-in function to get the optical flow map, \n",
        "            # given the previous frame, the current one.\n",
        "            flow = cv2.calcOpticalFlowFarneback(prev_frame, frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "            # Output is a numpy array with 2 channels, \n",
        "            # [0] is the optical flow map on the x axis \n",
        "            # Reshape the output optical map on the to have the channels first.\n",
        "            flow_x = np.reshape(flow[:, :, 0], (1, 60, 40))\n",
        "            # Append the calculated maps to the putput volume.\n",
        "            volume = np.concatenate((volume, flow_x),  axis=0)\n",
        "        prev_frame = frame\n",
        "\n",
        "    prev_frame = None\n",
        "    for frame in cur_volume:\n",
        "        # For every 2 fames calculate the optical flow map between them.\n",
        "        if prev_frame is not None:\n",
        "            # Using the cv2 built-in function to get the optical flow map, \n",
        "            # given the previous frame, the current one.\n",
        "            flow = cv2.calcOpticalFlowFarneback(prev_frame, frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "            # Output is a numpy array with 2 channels, \n",
        "            # [1] is the optical flow map on the y-axis.\n",
        "            # Reshape the output optical map on the to have the channels first.\n",
        "            flow_y = np.reshape(flow[:, :, 1], (1, 60, 40))\n",
        "            # Append the calculated maps to the putput volume.\n",
        "            volume = np.concatenate((volume, flow_y),  axis=0)\n",
        "        prev_frame = frame\n",
        "\n",
        "    # print(volume.shape)\n",
        "    return volume"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHPp3YgPIhgb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(input_videos[0][0].shape)\n",
        "#tmp = H1(np.asarray([input_videos[0][0]]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnHW5wXL5GjO",
        "colab_type": "text"
      },
      "source": [
        "## First Convolutional Layer\n",
        "\n",
        "* On each channel apply a kernal **(7x7x3)** twice.\n",
        "*(7x7 in spatial domain, 3 in the temporal)\n",
        "\n",
        "**Input (33@60x40)**\n",
        "\n",
        "**Output (2*23@54x34)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iUwPO82A8ZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def C2(input):\n",
        "    out_set = None\n",
        "    print(\"C2 volumes:\")\n",
        "    # kernel_size(depth, (kernel_shape))\n",
        "    gray   = Conv3D(1, activation='tanh', kernel_size=(3, 7, 7), padding='valid', \n",
        "                 input_shape=(7, 60, 40, 1), data_format='channels_last')(crop(1, 0, 7)(input))\n",
        "    print(\"gray\", gray.shape)\n",
        "    grad_x = Conv3D(1, activation='tanh', kernel_size=(3, 7, 7), padding='valid', \n",
        "                 input_shape=(7, 60, 40, 1), data_format='channels_last')(crop(1, 7, 14)(input))\n",
        "    print(\"grad-x\", grad_x.shape)\n",
        "    grad_y = Conv3D(1, activation='tanh', kernel_size=(3, 7, 7), padding='valid', \n",
        "                 input_shape=(7, 60, 40, 1), data_format='channels_last')(crop(1, 14, 21)(input))\n",
        "    print(\"grad-y\", grad_y.shape)\n",
        "    opt_x  = Conv3D(1, activation='tanh', kernel_size=(3, 7, 7), padding='valid', \n",
        "                 input_shape=(6, 60, 40, 1), data_format='channels_last')(crop(1, 21, 27)(input))\n",
        "    print(\"opt-x\", opt_x.shape)\n",
        "    opt_y  = Conv3D(1, activation='tanh', kernel_size=(3, 7, 7), padding='valid', \n",
        "                 input_shape=(6, 60, 40, 1), data_format='channels_last')(crop(1, 27, 33)(input))\n",
        "    print(\"opt-y\", opt_y.shape)    \n",
        "    # concatencation of the 5 channels is done on the second/dim-1 \n",
        "    # which is the dimenion of frames, all are put in one set.\n",
        "    out_set = concatenate([gray, grad_x, grad_y, opt_x, opt_y], axis = 1)\n",
        "    print(\"out set:\", out_set.shape)    \n",
        "    return out_set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTmf4Hx66hCf",
        "colab_type": "text"
      },
      "source": [
        "## Apply subsampling **(max-pooling 2x2)**\n",
        "\n",
        "**Output (2*23@27x17)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0dCc0-u6vfj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to conserve the depth, to not be affected by the downsampling, we but it = 1.\n",
        "def S3(input):\n",
        "    out_set = MaxPooling3D(pool_size=(1, 2, 2))(input)\n",
        "    return out_set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN60CYQu6wkV",
        "colab_type": "text"
      },
      "source": [
        "## Second Convolutional Layer\n",
        "\n",
        "* On each channel apply a kernal **(7x6x3)** thrice.\n",
        "\n",
        "**Output (6*13@21x12)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0tZlB-BZZQ8a",
        "colab": {}
      },
      "source": [
        "def C4(input):\n",
        "    out_set = None \n",
        "    print(\"C4 volumes:\")\n",
        "    # kernel_size(depth, (kernel_shape))\n",
        "    gray   = Conv3D(1, activation='tanh', kernel_size=(3, 7, 6), padding='valid', \n",
        "                 input_shape=(5, 27, 17, 1), data_format='channels_last')(crop(1, 0, 5)(input))\n",
        "    print(\"gray\", gray.shape)\n",
        "    grad_x = Conv3D(1, activation='tanh', kernel_size=(3, 7, 6), padding='valid', \n",
        "                 input_shape=(5, 27, 17, 1), data_format='channels_last')(crop(1, 5, 10)(input))\n",
        "    print(\"grad-x\", grad_x.shape)\n",
        "    grad_y = Conv3D(1, activation='tanh', kernel_size=(3, 7, 6), padding='valid', \n",
        "                 input_shape=(5, 27, 17, 1), data_format='channels_last')(crop(1, 10, 15)(input))\n",
        "    print(\"grad-y\", grad_y.shape)\n",
        "    opt_x  = Conv3D(1, activation='tanh', kernel_size=(3, 7, 6), padding='valid', \n",
        "                 input_shape=(4, 27, 17, 1), data_format='channels_last')(crop(1, 15, 19)(input))\n",
        "    print(\"opt-x\", opt_x.shape)\n",
        "    opt_y  = Conv3D(1, activation='tanh', kernel_size=(3, 7, 6), padding='valid', \n",
        "                 input_shape=(4, 27, 17, 1), data_format='channels_last')(crop(1, 19, 23)(input))\n",
        "    print(\"opt-y\", opt_y.shape)    \n",
        "    # concatencation of the 5 channels is done on the second/dim-1 \n",
        "    # which is the dimenion of frames, all are put in one set.\n",
        "    out_set = concatenate([gray, grad_x, grad_y, opt_x, opt_y], axis = 1)\n",
        "    print(\"out set:\", out_set.shape) \n",
        "    return out_set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb1A9Nrr7QjM",
        "colab_type": "text"
      },
      "source": [
        "## Apply subsampling **(max-pooling 3x3)**\n",
        "\n",
        "**Output (6*13@7x4)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSLZo2_A7Y11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def S5(input):\n",
        "    output = MaxPooling3D(pool_size=(1, 3, 3))(input)\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqWGduWl7Zw0",
        "colab_type": "text"
      },
      "source": [
        "## Third Convolutional Layer\n",
        "    \n",
        "* On each channel apply a kernal **(7x4)**\n",
        "\n",
        "**Output (78@1x1)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQRjtrHm71E3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def C6(input):\n",
        "    out_set = None \n",
        "    print(\"C6 volumes:\")\n",
        "    # kernel_size(depth, (kernel_shape))\n",
        "    gray   = Conv3D(1, activation='tanh', kernel_size=(1, 7, 4), padding='valid', \n",
        "                 input_shape=(3, 7, 4, 1), data_format='channels_last')(crop(1, 0, 3)(input))\n",
        "    print(\"gray\", gray.shape)\n",
        "    grad_x = Conv3D(1, activation='tanh', kernel_size=(1, 7, 4), padding='valid', \n",
        "                 input_shape=(3, 7, 4, 1), data_format='channels_last')(crop(1, 3, 6)(input))\n",
        "    print(\"grad-x\", grad_x.shape)\n",
        "    grad_y = Conv3D(1, activation='tanh', kernel_size=(1, 7, 4), padding='valid', \n",
        "                 input_shape=(3, 7, 4, 1), data_format='channels_last')(crop(1, 6, 9)(input))\n",
        "    print(\"grad-y\", grad_y.shape)\n",
        "    opt_x  = Conv3D(1, activation='tanh', kernel_size=(1, 7, 4), padding='valid', \n",
        "                 input_shape=(2, 7, 4, 1), data_format='channels_last')(crop(1, 9, 11)(input))\n",
        "    print(\"opt-x\", opt_x.shape)\n",
        "    opt_y  = Conv3D(1, activation='tanh', kernel_size=(1, 7, 4), padding='valid', \n",
        "                 input_shape=(2, 7, 4, 1), data_format='channels_last')(crop(1, 11, 13)(input))\n",
        "    print(\"opt-y\", opt_y.shape)    \n",
        "    # concatencation of the 5 channels is done on the second/dim-1 \n",
        "    # which is the dimenion of frames, all are put in one set.\n",
        "    out_set = concatenate([gray, grad_x, grad_y, opt_x, opt_y], axis = 1)\n",
        "    print(\"out set:\", out_set.shape) \n",
        "    return out_set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWYmqjv-78S1",
        "colab_type": "text"
      },
      "source": [
        "## FC-Layer of **(128@1x1)**\n",
        "* Connect to output layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rfGEdsI8Rqz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def FC(input:list):\n",
        "    # input: a list of sets representing the last output from layer C6.\n",
        "    output = Flatten()(input)\n",
        "    output = Dense(units=128, activation='tanh')(output)\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1tCNpXo1O2c",
        "colab_type": "text"
      },
      "source": [
        "# Variables\n",
        "\n",
        "* Input:\n",
        "    * input_videos : list of vedios, dimensions (m, n, frame_rows, frame_cols).\n",
        "    \n",
        "    * data : numpy array, dimensions (m, n, 7, frame_rows, frame_cols).\n",
        "    * m : #Vedios, n : #Frames in a vedio, (frame_rows, frame_cols) : Frame dimensions (40x60)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZj4-vYf4emQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input_videos = np.asarray(input_videos)\n",
        "step = 6\n",
        "inside_step = 2\n",
        "\n",
        "# drunkly verified\n",
        "num_of_volumes = ((input_videos[0].shape[0] - (step * inside_step))//step) + 1\n",
        "train_data_shape = (input_videos.shape[0], num_of_volumes)\n",
        "\n",
        "# N: \"number videos\" * \"number volumes\"\n",
        "N = input_videos.shape[0] * num_of_volumes\n",
        "x_train = np.empty((input_videos.shape[0], num_of_volumes, 33, 60, 40), dtype=object)\n",
        "y_train = np.empty((input_videos.shape[0], num_of_volumes, 2), dtype=object)\n",
        "\n",
        "# The loop\n",
        "for vid_ind in range(len(input_videos)):\n",
        "    for frame_ind, vol_num in zip(range(step, len(input_videos[vid_ind]) - step, step), range(num_of_volumes)):\n",
        "        first_frame = frame_ind - 6\n",
        "        last_frame  = frame_ind + 6 + 1 # adding one for slicing\n",
        "\n",
        "        # get the new volume from the hardwired level.\n",
        "        #new_volume  = input_videos[vid_ind, first_frame:last_frame:inside_step, :, :]\n",
        "        new_volume = H1(input_videos[vid_ind, first_frame:last_frame:inside_step, :, :])\n",
        "        \n",
        "        # Add the new volume to input of the first 3D-CNN layer\n",
        "        x_train[vid_ind, vol_num, :, :, :] = new_volume\n",
        "\n",
        "        # Create the label for this volume.\n",
        "        # video_label carries the video label, 1 for truthful and 0 for deceptive.\n",
        "        # our y_train has 2 cells, [0] deceptive and [1] for truthful\n",
        "        cur_y = np.zeros((2), int)\n",
        "        cur_y[video_label[vid_ind]] = 1\n",
        "        #print(cur_y.shape)\n",
        "        y_train[vid_ind, vol_num, :] = cur_y\n",
        "\n",
        "# Reshape the data to (N, dimesions)\n",
        "x_train = x_train.reshape((N, 33, 60, 40, 1))\n",
        "y_train = y_train.reshape((N, 2))\n",
        "\n",
        "print(x_train.shape, y_train.shape)\n",
        "# print(y_train[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "408dahDhjTOm",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tz1yhBCav1u9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}